# ============================================================================
# PostgreSQL MCP Server - Environment Configuration
# ============================================================================
# Copy this file to .env and configure with your actual values
# DO NOT commit .env to version control - it contains sensitive credentials
# ============================================================================

# ============================================================================
# DATABASE CONFIGURATION (Multi-Database Support)
# ============================================================================
# PostgreSQL database connection settings
# Configure multiple databases using indexed environment variables:
# DATABASES__0__NAME, DATABASES__0__HOST, etc. for first database
# DATABASES__1__NAME, DATABASES__1__HOST, etc. for second database

# First Database (blog_small)
DATABASES__0__NAME=blog_small
DATABASES__0__HOST=localhost
DATABASES__0__PORT=5432
DATABASES__0__DATABASE=blog_small
DATABASES__0__USER=postgres
DATABASES__0__PASSWORD=postgres
DATABASES__0__MIN_POOL_SIZE=5
DATABASES__0__MAX_POOL_SIZE=20
DATABASES__0__POOL_TIMEOUT=30
DATABASES__0__COMMAND_TIMEOUT=30

# Second Database (ecommerce_medium) - Optional
# Uncomment to enable multiple database support
# DATABASES__1__NAME=ecommerce_medium
# DATABASES__1__HOST=localhost
# DATABASES__1__PORT=5432
# DATABASES__1__DATABASE=ecommerce_medium
# DATABASES__1__USER=postgres
# DATABASES__1__PASSWORD=postgres
# DATABASES__1__MIN_POOL_SIZE=5
# DATABASES__1__MAX_POOL_SIZE=20
# DATABASES__1__POOL_TIMEOUT=30
# DATABASES__1__COMMAND_TIMEOUT=30

# Third Database (saas_crm_large) - Optional
# DATABASES__2__NAME=saas_crm_large
# DATABASES__2__HOST=localhost
# DATABASES__2__PORT=5432
# DATABASES__2__DATABASE=saas_crm_large
# DATABASES__2__USER=postgres
# DATABASES__2__PASSWORD=postgres
# DATABASES__2__MIN_POOL_SIZE=5
# DATABASES__2__MAX_POOL_SIZE=20
# DATABASES__2__POOL_TIMEOUT=30
# DATABASES__2__COMMAND_TIMEOUT=30

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================
# OpenAI API settings for LLM-powered SQL generation and validation

# OpenAI API key (required)
# Get your key from: https://platform.openai.com/api-keys
# SECURITY: Never commit this to version control
# OPENAI_API_KEY=sk-your-openai-api-key-here

# OpenAI model to use for SQL generation
# Recommended models:
#   - gpt-5.2-mini: Fast, cost-effective, good for SQL generation
#   - gpt-4o: More accurate for complex queries, higher cost
#   - gpt-4o-mini: Balance of speed and accuracy
OPENAI_MODEL=gpt-5.2-mini

# Maximum tokens for LLM requests
# Controls the maximum length of generated SQL and responses
# gpt-5.2-mini supports up to 128k tokens context
# Recommended: 32000 for most use cases
OPENAI_MAX_TOKENS=32000

# Model temperature (0.0 = deterministic, 2.0 = creative)
# For SQL generation, use 0.0 for consistent, predictable outputs
# Higher values introduce randomness (not recommended for SQL)
OPENAI_TEMPERATURE=0.0

# API request timeout in seconds
# How long to wait for OpenAI API responses
# Recommended: 30-60 seconds
OPENAI_TIMEOUT=30

# ============================================================================
# SECURITY CONFIGURATION
# ============================================================================
# Security settings to protect your database from unauthorized operations

# Allow write operations (INSERT, UPDATE, DELETE, TRUNCATE)
# SECURITY: Keep this false in production to enforce read-only access
# Only enable if you explicitly need write capabilities
SECURITY_ALLOW_WRITE_OPERATIONS=false

# Comma-separated list of blocked table names (or schema.table format)
# Tables in this list cannot be accessed via queries
# Example: users,public.secrets,audit_logs
# SECURITY: Add sensitive tables that should never be queried
SECURITY_BLOCKED_TABLES=

# Comma-separated list of blocked column names (or table.column format)
# Columns in this list cannot be accessed via queries
# Example: password,ssn,credit_card,users.api_key
# SECURITY: Add sensitive columns that should never be exposed
SECURITY_BLOCKED_COLUMNS=

# Comma-separated list of blocked PostgreSQL functions
# These functions are security risks and will be rejected during validation
# Includes: file I/O, sleep functions, admin functions, etc.
# Add additional functions as needed for your security requirements
SECURITY_BLOCKED_FUNCTIONS=pg_sleep,pg_read_file,pg_write_file,lo_import,lo_export,pg_ls_dir,pg_read_binary_file,pg_stat_file,copy_from_program,copy_to_program

# Allow EXPLAIN queries
# EXPLAIN queries can reveal schema information and query plans
# SECURITY: Keep this false unless you need query analysis capabilities
SECURITY_ALLOW_EXPLAIN=false

# Maximum number of rows returned per query
# Prevents memory exhaustion from extremely large result sets
# Recommended: 1000-10000 depending on your use case
SECURITY_MAX_ROWS=10000

# Maximum query execution time in seconds
# Queries exceeding this time will be cancelled
# Recommended: 30-60 seconds
SECURITY_MAX_EXECUTION_TIME=30

# PostgreSQL role to switch to for read-only access (optional)
# If specified, the server will execute SET ROLE before each query
# This provides an additional layer of security enforcement
# The role should have read-only permissions
# Example: readonly_user
SECURITY_READONLY_ROLE=

# Safe search_path for query execution
# Limits schema access to specified schemas
# Recommended: public (or your specific schema names)
SECURITY_SAFE_SEARCH_PATH=public

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
# Settings for input validation and result verification

# Maximum question length in characters
# Prevents excessively long input that could cause issues
# Recommended: 5000-10000 characters
VALIDATION_MAX_QUESTION_LENGTH=10000

# Minimum confidence score for accepting query results (0-100)
# Results below this score will be flagged but still returned
# Higher values = stricter validation, may reject more queries
# Recommended: 70-80 for most use cases
VALIDATION_MIN_CONFIDENCE_SCORE=70

# ============================================================================
# CACHE CONFIGURATION
# ============================================================================
# Schema caching settings to improve performance

# Enable schema caching
# When enabled, database schema is cached to reduce metadata queries
# Recommended: true for production
CACHE_ENABLED=true

# Schema cache Time-To-Live in seconds
# How long to keep cached schema before refreshing
# Recommended: 3600 (1 hour) for mostly static schemas
# Use lower values if schema changes frequently
CACHE_SCHEMA_TTL=3600

# Maximum number of schemas to cache
# Controls memory usage for multi-database deployments
# Recommended: 100 (more than enough for most use cases)
CACHE_MAX_SIZE=100

# ============================================================================
# RESILIENCE CONFIGURATION
# ============================================================================
# Settings for retry logic, circuit breakers, and fault tolerance

# Maximum number of retry attempts for failed operations
# Applies to LLM API calls and transient database errors
# Recommended: 3-5 retries
RESILIENCE_MAX_RETRIES=3

# Initial retry delay in seconds
# Time to wait before first retry attempt
# Recommended: 1.0-2.0 seconds
RESILIENCE_RETRY_DELAY=1.0

# Exponential backoff factor for retries
# Each retry waits (delay * factor^attempt) seconds
# Example with factor=2.0: 1s, 2s, 4s, 8s
# Recommended: 2.0 for exponential backoff
RESILIENCE_BACKOFF_FACTOR=2.0

# Circuit breaker failure threshold
# Number of consecutive failures before opening circuit
# When circuit is open, requests fail fast without attempting operation
# Recommended: 5-10 failures
RESILIENCE_CIRCUIT_BREAKER_THRESHOLD=5

# Circuit breaker timeout in seconds
# How long to wait before attempting to close circuit (test if service recovered)
# Recommended: 60-120 seconds
RESILIENCE_CIRCUIT_BREAKER_TIMEOUT=60

# ============================================================================
# OBSERVABILITY CONFIGURATION
# ============================================================================
# Settings for metrics, logging, and monitoring

# Enable Prometheus metrics endpoint
# When enabled, metrics are exposed on the metrics port
# Recommended: true for production monitoring
OBSERVABILITY_METRICS_ENABLED=true

# Port for Prometheus metrics HTTP server
# Make sure this port is available and not blocked by firewall
# Recommended: 9090 (standard Prometheus port) or any available port
OBSERVABILITY_METRICS_PORT=9090

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
# DEBUG: Verbose logging for development
# INFO: Standard logging for production
# WARNING: Only warnings and errors
# ERROR: Only errors and critical issues
# Recommended: INFO for production, DEBUG for development
OBSERVABILITY_LOG_LEVEL=INFO

# Log format
# Options: json, text
# json: Structured JSON logs (recommended for production, works with log aggregators)
# text: Human-readable text logs (easier for local development)
# Recommended: json for production, text for development
OBSERVABILITY_LOG_FORMAT=json

# ============================================================================
# ENVIRONMENT
# ============================================================================
# Application environment identifier

# Environment name (development, staging, production)
# Used for conditional behavior and logging context
# Recommended: Set appropriately for each deployment
ENVIRONMENT=development

# ============================================================================
# CONFIGURATION VALIDATION CHECKLIST
# ============================================================================
# Before running the server, verify:
# [ ] Database credentials are correct (test with: psql -h $HOST -U $USER -d $NAME)
# [ ] OpenAI API key is valid and has credits
# [ ] Security settings are appropriate for your use case
# [ ] Resource limits (max_rows, timeout) match your requirements
# [ ] Logging and metrics are configured for your monitoring setup
# [ ] .env file is NOT committed to version control (.gitignore contains .env)
#
# For production deployments, also verify:
# [ ] Using read-only database user
# [ ] SECURITY_ALLOW_WRITE_OPERATIONS=false
# [ ] Appropriate connection pool sizes for expected load
# [ ] Monitoring and alerting configured for metrics endpoint
# [ ] Secrets are stored in secure secret management system (not .env)
# [ ] Blocked tables and columns configured for sensitive data
# [ ] SECURITY_ALLOW_EXPLAIN=false for production environments

